{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VC Dimension, Halfspaces, and the Polynomial Kernel\n",
    "\n",
    "The perceptron hypothesis class is often called the hypothesis class of \"half spaces\" because it divides the space $R^d$ in half with a hyperplane.\n",
    "In this assignment, you will create various synthetic datasets, and explore how changing the hyperparameters of the halfspace hypothesis class with the polynomial kernel affects the statistical performance.\n",
    "The advantage of using synthetic data is that you can control various aspects of the data generation process to see how different sources of error result in different types of output.\n",
    "\n",
    "The main purpose of this assignment is to help give you intuition for all of the terms and formulas we've been defining in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONWARNINGS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# import data mining libraries\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# import and configure plotting\u001b[39;00m\n\u001b[1;32m     25\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell contains imports and global configurations.\n",
    "You shouldn't have to modify anything in this cell.\n",
    "'''\n",
    "# import standard python libraries libraries\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "import time\n",
    "\n",
    "# disable warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "# import data mining libraries\n",
    "import sklearn.linear_model\n",
    "\n",
    "# import and configure plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['lines.markersize'] = 1\n",
    "#plt.rcParams['figure.figsize'] = [9, 6]\n",
    "#plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "# configurations for my code\n",
    "default_num_trials = 25\n",
    "max_d = 2**14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Exploring the VC Dimension of the polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_kernel_embedding(x, p):\n",
    "    '''\n",
    "    Embeds the vector x into a higher dimensional space using the polynomial kernel of degree p.\n",
    "    The output dimension is equal to the VC-dimension of Theta(min{p**d,d**p}), where d is the dimension of x.\n",
    "    This is not an efficient implementation; it is designed for clarity.\n",
    "    \n",
    "    >>> polynomial_kernel_embedding(np.array([2]),1)\n",
    "    array([2])\n",
    "    >>> polynomial_kernel_embedding(np.array([2]),2)\n",
    "    array([2, 4])\n",
    "    >>> polynomial_kernel_embedding(np.array([2]),3)\n",
    "    array([2, 4, 8])\n",
    "    \n",
    "    >>> polynomial_kernel_embedding(np.array([2,3]),1)\n",
    "    array([2, 3])\n",
    "    >>> polynomial_kernel_embedding(np.array([2,3]),2)\n",
    "    array([2, 3, 4, 6, 9])\n",
    "    >>> polynomial_kernel_embedding(np.array([2,3]),3)\n",
    "    array([ 2,  3,  4,  6,  9,  8, 12, 18, 27])\n",
    "    \n",
    "    >>> polynomial_kernel_embedding(np.array(range(2)),10).shape\n",
    "    (65,)\n",
    "    >>> polynomial_kernel_embedding(np.array(range(3)),10).shape\n",
    "    (285,)\n",
    "    >>> polynomial_kernel_embedding(np.array(range(4)),10).shape\n",
    "    (1000,)\n",
    "    '''\n",
    "    assert type(x) is np.ndarray\n",
    "    assert len(x.shape) == 1\n",
    "    assert type(p) is int\n",
    "    assert p > 0\n",
    "    \n",
    "    d = x.shape[0]\n",
    "    terms_per_degree = [ [ [i] for i in range(d) ] ]\n",
    "    for i in range(1,p):\n",
    "        deg_i_terms = []\n",
    "        deg_i_minus1_terms = terms_per_degree[-1]\n",
    "        for k in range(len(deg_i_minus1_terms)):\n",
    "            for j in range(deg_i_minus1_terms[k][-1],d):\n",
    "                deg_i_terms.append(deg_i_minus1_terms[k]+[j])\n",
    "        terms_per_degree.append(deg_i_terms)\n",
    "    \n",
    "    terms = [ inner for outer in terms_per_degree for inner in outer ]\n",
    "    \n",
    "    values = []\n",
    "    for term in terms:\n",
    "        value = 1\n",
    "        for i in term:\n",
    "            value *= x[i]\n",
    "        values.append(value)\n",
    "    return np.array(values)\n",
    "    \n",
    "    embeddings = [x]\n",
    "    for i in range(1,p):\n",
    "        deg_i_terms = []\n",
    "        deg_i_minus1_terms = embeddings[-1]\n",
    "        for j in range(x.shape[0]):\n",
    "            for k in range(deg_i_minus1_terms.shape[0]):\n",
    "                deg_i_terms.append(x[j]*deg_i_minus1_terms[k])\n",
    "        embeddings.append(np.array(deg_i_terms))\n",
    "    return np.concatenate(embeddings)\n",
    "\n",
    "import doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This cell plots the VC-dimension of a halfspace with the polynomial kernel\n",
    "as a function of the degree p with a fixed dimension d\n",
    "'''\n",
    "ps = range(1,50)\n",
    "d = 2\n",
    "vcdims = [ polynomial_kernel_embedding(np.ones([d]),p).shape for p in ps ]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.ylabel('VC-dimension')\n",
    "plt.xlabel('polynomial degree = p')\n",
    "plt.plot(ps,vcdims)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell plots the VC-dimension of a halfspace with the polynomial kernel\n",
    "as a function of the dimension d with a fixed p\n",
    "'''\n",
    "ds = range(1,50)\n",
    "p = 2\n",
    "vcdims = [ polynomial_kernel_embedding(np.ones([d]),p).shape for d in ds ]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.ylabel('VC-dimension')\n",
    "plt.xlabel('input dimension = d')\n",
    "plt.plot(ps,vcdims)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell plots the VC-dimension of a halfspace with the polynomial kernel\n",
    "as the dimension and degree both increase\n",
    "'''\n",
    "dps = range(1,10)\n",
    "vcdims = [ polynomial_kernel_embedding(np.ones([dp]),dp).shape for dp in dps ]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.ylabel('VC-dimension')\n",
    "plt.xlabel('polynomial degree = p = input dimension = d')\n",
    "plt.plot(dps,vcdims)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1\n",
    "*The plots above visualize the VC-dimension of halfspaces with the polynomical kernel.\n",
    "In particular, they show regimes where the VC-dimension grows polynomially and regimes where it grows exponentially.*\n",
    "\n",
    "*What is the formula for the VC-dimension of halfspaces with the polynomial kernel?*\n",
    "\n",
    "Type your answers below each question in non-italic text.\n",
    "\n",
    "d_vc = p+d choose p = (p+d)!/(p!*d!) = theta(d^p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2\n",
    "*In most practical problems, the dimension $d$ of the input dataspace $\\mathcal X$ is large ( think $d > 10^{6}$).\n",
    "Large degree polynomial kernels ($p>3$) are not typically used in this situation.\n",
    "Explain the downsides of using a large degree polynomial kernel in terms of sample complexity using VC theory.*\n",
    "\n",
    "If we use a large degree polynomial kernel with an already large original dimension, then the VC dimension increases drastically (as can be seen in the formula for vc dimension above: if d = 10^6, and we use a large degree polynomial kernel say p=4, then d_vc = theta(10^24)). What this extremely high vc dimension indicates is that the model has a much greated capacity to shatter complex data sets, which indicates an increased likelihood of overfitting. According to VC theory, if we want a good generalization error, then the amount of data points N should be proportional to the vc dimension: N = roughly 10*d_vc. Therefore, if we want to achieve a satisfactory generalization error whilst using large degree polynomial kernel on top of an already large dimension, then we would need an astronomically large data set (in the previous example, N would have to be 10^25), which is really difficult. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: generating synthetic data with different properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell contains a number of functions for generating and visualizing datasets.\n",
    "'''\n",
    "\n",
    "def generate_dataset(m, d, f, sigma, seed=0):\n",
    "    '''\n",
    "    Returns a dataset with m data points of dimension d generated by:\n",
    "    \n",
    "        X ~ Uniform(-1, 1)\n",
    "        Y = f(X) + epsilon, where epsilon ~ Normal(0,sigma)\n",
    "        \n",
    "    When sigma=0, there is no randomness in Y, and so the bayes error will be 0\n",
    "    When sigma>0, there is randomness in Y, and so the bayes error > 0\n",
    "    '''\n",
    "    # set numpy's seed for reproducable results\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # ensure reasonable input parameters\n",
    "    assert type(m) is int\n",
    "    assert m>0\n",
    "    \n",
    "    # a helper function for generating labels\n",
    "    def sign(a):\n",
    "        if a>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    # generate the features\n",
    "    X = np.random.uniform(low=-1.0,high=1.0,size=[m,d])\n",
    "\n",
    "    # generate the labels\n",
    "    Ys = []\n",
    "    for i in range(m):\n",
    "        epsilon = np.random.randn()*sigma\n",
    "        yi = sign(f(X[i]) + epsilon)\n",
    "        Ys.append(yi)\n",
    "    Y = np.array(Ys)\n",
    "    \n",
    "    # return the dataset\n",
    "    return (X,Y)\n",
    "\n",
    "\n",
    "def f_polynomial(p, seed=0):\n",
    "    '''\n",
    "    Implements a polynomial embedding of degree $p$ to perform the data labeling.\n",
    "    If the halfspace hypothesis class with the polynomial kernel of degree > p is used for learning,\n",
    "    then the hypothesis class will be realizable.\n",
    "    The input dimension $d$ must match the dimension of the dataspace $\\mathcal X$\n",
    "    '''\n",
    "    phi = lambda x: polynomial_kernel_embedding(x, p)\n",
    "    embedding_dim = phi(np.ones([d])).shape[0]\n",
    "    np.random.seed(seed)\n",
    "    #w = (np.array(range(embedding_dim))/embedding_dim)**0.01\n",
    "    #w = np.ones([embedding_dim])/embedding_dim\n",
    "    w = np.random.randn(max_d)/math.sqrt(embedding_dim)\n",
    "    def f(x):\n",
    "        phi_x = phi(x)\n",
    "        w2 = w[:phi_x.shape[0]]\n",
    "        return phi_x.transpose() @ w2\n",
    "    return f\n",
    "    \n",
    "    \n",
    "def f_checkers():\n",
    "    '''\n",
    "    This is another embedding function that creates a \"checkers\" pattern in the data.\n",
    "    This embedding is not realizable for halfspaces with the polynomial kernel. \n",
    "    '''\n",
    "    p = 4\n",
    "    def f(x):\n",
    "        return math.sin(x[0]*p)*math.sin(x[1]*p)\n",
    "    return f\n",
    "\n",
    "    \n",
    "def f_circles():\n",
    "    '''\n",
    "    This is another embedding function that creates a pattern of concentric circles in the data.\n",
    "    This embedding is not realizable for halfspaces with the polynomial kernel. \n",
    "    '''\n",
    "    p = 4\n",
    "    def f(x):\n",
    "        return math.cos((x[0]*x[0]+x[1]*x[1])*p)\n",
    "    return f\n",
    "    \n",
    "\n",
    "def plot_dataset(S):\n",
    "    '''\n",
    "    Plots the first two dimensions of the input dataset.\n",
    "    '''\n",
    "    X,Y = S\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,0],X[:,1],c=Y)\n",
    "    #ax.scatter(X_pos[:,0], X_pos[:,1])\n",
    "    #ax.scatter(X_neg[:,0], X_neg[:,1])\n",
    "    ax.set_xlim(-1.0,1.0)\n",
    "    ax.set_ylim(-1.0,1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/6c2b_mg91nggzny1k_1lkkdr0000gn/T/ipykernel_3331/749359359.py:46: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  '''\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      9\u001b[0m sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mf_polynomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# generate and plot dataset\u001b[39;00m\n\u001b[1;32m     13\u001b[0m S \u001b[38;5;241m=\u001b[39m generate_dataset(m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m16\u001b[39m,d\u001b[38;5;241m=\u001b[39md,f\u001b[38;5;241m=\u001b[39mf,sigma\u001b[38;5;241m=\u001b[39msigma)\n",
      "Cell \u001b[0;32mIn[2], line 53\u001b[0m, in \u001b[0;36mf_polynomial\u001b[0;34m(p, seed)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03mImplements a polynomial embedding of degree $p$ to perform the data labeling.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mIf the halfspace hypothesis class with the polynomial kernel of degree > p is used for learning,\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mthen the hypothesis class will be realizable.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mThe input dimension $d$ must match the dimension of the dataspace $\\mathcal X$\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     52\u001b[0m phi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: polynomial_kernel_embedding(x, p)\n\u001b[0;32m---> 53\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m phi(\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mones([d]))\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     54\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#w = (np.array(range(embedding_dim))/embedding_dim)**0.01\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#w = np.ones([embedding_dim])/embedding_dim\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell plots a dataset that has no randomness in the generation of class labels (sigma=0).\n",
    "Using our notation from the textbook, this means that for each i, y_i = f(x_i).\n",
    "When p=1 in the f_polynomial function, the data has a linear decision boundary.\n",
    "'''\n",
    "\n",
    "# these \"hyperparameters\" control the properties of the dataset\n",
    "d = 2\n",
    "sigma = 0\n",
    "f = f_polynomial(p=1,seed=2)\n",
    "\n",
    "# generate and plot dataset\n",
    "S = generate_dataset(m=2**16,d=d,f=f,sigma=sigma)\n",
    "plot_dataset(S)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "\n",
    "*Regenerate the dataset above several times with degree $1 \\le p \\le 10$ to explore what decision boundaries for higher degree polynomials look like.\n",
    "You should also change the `seed` value to generate different datasets with the same degree.*\n",
    "\n",
    "*Write a 1-2 sentence summary of your findings.*\n",
    "\n",
    "By increasing p's value, the polynomial function captures more copmlex decision bounbdaries (when p=1, the decision boundary is linear vs. when p >3, the decision boundary becomes more and more irregular, which allows for nonlinear separation of dataset). When we change the seed value while maintaining the same polynomial degree, the complexity of the decision boundary is the same but as the seed value changes the randomness in the generation of data, the specific position/separation of the decision boundary changes (shift or rotate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell plots a dataset with randomness (sigma>0).\n",
    "The sigma variable controls the amount of randomness applied to the model's label. \n",
    "'''\n",
    "\n",
    "# these \"hyperparameters\" control the properties of the dataset\n",
    "d = 2\n",
    "sigma = 0.2\n",
    "f = f_polynomial(2)\n",
    "\n",
    "# generate and plot dataset\n",
    "S = generate_dataset(m=2**16,d=d,f=f,sigma=sigma)\n",
    "plot_dataset(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "\n",
    "*Experiment with different combinations of sigma (and different p values) to see how the level of randomness effects the data.*\n",
    "\n",
    "*Write a 1-2 sentence summary of your findings.*\n",
    "\n",
    "When we tune up the sigma value, there's way more noise in the data generated, thereby making the data much harder to separate linearly. Additionally, when we increase the dimensionality of the generated data, the level of randomness (i.e. the sigma value) needs to be way way smaller than it needs to be for a smaller dimension in order for the data to remain separable; in order words, increasing p dramatizes the effect of randomness in the data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell plots a dataset in the PAC model in higher dimensions.\n",
    "Notice that even though there is no randomness in the labels,\n",
    "there appears to be randomness when we only visualize the data in 2 dimensions.\n",
    "'''\n",
    "\n",
    "# these \"hyperparameters\" control the properties of the dataset\n",
    "d = 4\n",
    "sigma = 0\n",
    "f = f_polynomial(d,2)\n",
    "\n",
    "# generate and plot dataset\n",
    "S = generate_dataset(m=2**16,d=d,f=f,sigma=sigma)\n",
    "plot_dataset(S)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "\n",
    "*Experiment with different values of `d` in the dataset plotted above.\n",
    "Also experiment with different values of `p` and `sigma`.\n",
    "Notice that in high dimensions it is very difficult to tell if the data is linearly separable, or if the data contains any randomness.\n",
    "Write 1-2 sentences explaining why this is the case.*\n",
    "\n",
    "In high dimensions, the patterns become less clear/discernable since the data points are spread across many more dimensions, which also unfortunately masks any randomness in the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: plotting the error as a function of the sample size $m$\n",
    "\n",
    "NOTE: The textbook uses the variable $N$ to measure the total number of data points, but this notebook uses the variable $m$ to denote the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_err_vs_m(d, sigma, f, p, num_trials=default_num_trials):\n",
    "    '''\n",
    "    Plots the sample/true/generalization error for the halfspace with polynomial kernel model\n",
    "    as a function of the number of sample training points.\n",
    "    '''\n",
    "\n",
    "    max_m_exp = 16\n",
    "    m_buffer = 3\n",
    "    \n",
    "    # S_test is our test set used for measuring the model's performance;\n",
    "    # It has a very large size to ensure that the empirical risk on S_test is very close to the true risk\n",
    "    print('generating dataset... ',end='')\n",
    "    S_test = generate_dataset(m=2**(max_m_exp+m_buffer),d=d,f=f,sigma=sigma)\n",
    "    print('done')\n",
    "    plot_dataset(S_test)\n",
    "\n",
    "    # these lists store the computer training and test errors\n",
    "    test_errs = []\n",
    "    train_errs = []\n",
    "    \n",
    "    # This is the list of all sample sizes we will train models on and generate train/test errors;\n",
    "    # by adjusting the range(), you can adjust the x-axis in the plots below.\n",
    "    ms = [ 2**i for i in range(0,max_m_exp) ]\n",
    "\n",
    "    for m in ms:\n",
    "    #for p in [1,2,3,4,5,6,7]:\n",
    "\n",
    "        # In order to \"smooth\" the plots, we will repeat each experiment multiple times\n",
    "        # as deterimed by the num_trials parameter.\n",
    "        # These lists store the raw results from each trial.\n",
    "        trials_test_accs = []\n",
    "        trials_train_accs = []\n",
    "        \n",
    "        # loop over each trial\n",
    "        seed_base = 10\n",
    "        time_start = time.time()\n",
    "        for seed in range(seed_base,seed_base+num_trials):\n",
    "            \n",
    "            # generate a training set of size m\n",
    "            # from the same distribution as our test set;\n",
    "            # notice that we must explicitly set a unique seed for each trial so that\n",
    "            # each iteration is actually running on a different training set\n",
    "            S_train = generate_dataset(m=m,d=d,f=f,sigma=sigma,seed=seed)\n",
    "\n",
    "            try:   \n",
    "                # train a linear model;\n",
    "                # notice that all of these models use the same hypothesis set of half spaces;\n",
    "                # they only difference is that a different algorithm is used to select the hypothesis from the hypothesis class;\n",
    "                # VC theory doesn't care about the training algorithm used to select the hypothesis from the class,\n",
    "                # and so provides the same generalization bounds for all of these algorithms;\n",
    "                # in practice, they all perform essentially the same statistically,\n",
    "                # but they have different runtime characteristics in different situations\n",
    "                X, Y = S_train\n",
    "                X = np.apply_along_axis(lambda x: polynomial_kernel_embedding(x,p),1,X)\n",
    "                #h_S = sklearn.linear_model.LogisticRegression(solver='liblinear',C=1e10)\n",
    "                #h_S = sklearn.linear_model.Perceptron()\n",
    "                h_S = sklearn.linear_model.SGDClassifier(penalty=None)\n",
    "                #h_S = sklearn.linear_model.PassiveAggressiveClassifier()\n",
    "                #h_S = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "                #h_S = sklearn.svm.LinearSVC()\n",
    "                h_S.fit(X, Y)\n",
    "\n",
    "                # calculate the training accuracy\n",
    "                train_acc = h_S.score(X,Y)\n",
    "\n",
    "                # calculate the test accuracy\n",
    "                X, Y = S_test\n",
    "                X = X[:min(2048,m_buffer*m)]\n",
    "                Y = Y[:min(2048,m_buffer*m)]\n",
    "                X = np.apply_along_axis(lambda x: polynomial_kernel_embedding(x,p),1,X)\n",
    "                test_acc = h_S.score(X, Y)        \n",
    "\n",
    "            # ValueError raised when there's not enough data to perform classification;\n",
    "            # in this case, we get perfect training accuracy, but perfectly wrong test accuracy\n",
    "            except ValueError:\n",
    "                train_acc = 1\n",
    "                test_acc = 0\n",
    "                \n",
    "            trials_test_accs.append(test_acc)\n",
    "            trials_train_accs.append(train_acc)\n",
    "        time_end = time.time()\n",
    "        \n",
    "        # compute the average of our trials\n",
    "        train_acc = np.mean(trials_train_accs)\n",
    "        test_acc = np.mean(trials_test_accs)\n",
    "            \n",
    "        # print a debugging statement for each iteration\n",
    "        print('m=%8d,  train_acc=%0.4f,  test_acc=%0.4f,  time_diff=%dsec'%(\n",
    "            m,\n",
    "            train_acc,\n",
    "            test_acc,\n",
    "            time_end-time_start\n",
    "        ))\n",
    "\n",
    "        # convert the accuracies into errors and store them\n",
    "        train_errs.append(1-train_acc)\n",
    "        test_errs.append(1-test_acc)\n",
    "    \n",
    "    # plot the errors\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(14,5))\n",
    "    ax1.set_xscale('log',base=2)\n",
    "    #ax1.set_yscale('log')\n",
    "    ax1.set_ylim([0.0,1.0])\n",
    "    ax1.set(\n",
    "        xlabel='number of samples = m', \n",
    "        ylabel='train error = E_in(g)',\n",
    "    )\n",
    "    ax1.plot(ms,train_errs)\n",
    "    \n",
    "    ax2.set_xscale('log',base=2)\n",
    "    #ax2.set_yscale('log')\n",
    "    ax2.set_ylim([0.0,1.0])\n",
    "    ax2.set(\n",
    "        xlabel='number of samples = m', \n",
    "        ylabel='test error ≈ E_out(g)',\n",
    "    )\n",
    "    ax2.plot(ms,test_errs)\n",
    "    \n",
    "    ax3.set_xscale('log',base=2)\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.set(\n",
    "        xlabel='number of samples = m', \n",
    "        ylabel='generalization error = |E_in(g) - E_out(g)|',\n",
    "\n",
    "    )\n",
    "    ax3.plot(ms,np.abs(np.array(test_errs)-np.array(train_errs)))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err_vs_m(\n",
    "    d=2,\n",
    "    sigma=0,\n",
    "    f=f_polynomial(p=2),\n",
    "    p=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err_vs_m(\n",
    "    d=2,\n",
    "    sigma=0,\n",
    "    f=f_polynomial(p=2),\n",
    "    p=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err_vs_m(\n",
    "    d=2,\n",
    "    sigma=0.2,\n",
    "    f=f_polynomial(p=2),\n",
    "    p=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "We briefly introduce some common definitions that are not found in the textbook and we have not directly discussed in class.\n",
    "These definitions will help us more succinctly state the following questions.\n",
    "Another purpose of these definitions is to give you practice working with new definitions that you haven't seen explained before.\n",
    "Data mining literature very frequently introduces new definitions that will be closely related,\n",
    "but subtly different to definitions you've used before.\n",
    "\n",
    "We call the \"approximation error\" of a hypothesis class to be the true error of the best possible model in the hypothesis class.\n",
    "That is, the approximation error is\n",
    "$$\n",
    "E_{approx}(\\mathcal{H}) = \\text{argmin}_{h\\in \\mathcal H} E_{out}(h).\n",
    "$$\n",
    "Notice that the approximation error depends both on the hypothesis class chosen for learning, but also on the data distribution (since $E_{out}$ depends on the data distribution).\n",
    "It does not, however, depend on the training data.\n",
    "\n",
    "We call a hypothesis class \"realizable\" when the approximation error is 0.\n",
    "That is, there exists some $h \\in \\mathcal H$ with $E_{out}(h) = 0$.\n",
    "Notice that this definition is independent of the training data and the training algorithm.\n",
    "\n",
    "### Question 3.1\n",
    "\n",
    "*The three cells above plot the accuracy of the polynomial kernel in three situations:\n",
    "when the model is not realizable because the degree of the polynomial is too small,\n",
    "when the model is realizable,\n",
    "and when the model is not realizable because of randomness in the labeling process.*\n",
    "\n",
    "*Write 1-2 sentences about the differences you observe in each of these cases and how they relate to VC theory.*\n",
    "\n",
    "As expected, all three errors is lowest in the scenario where the model is realizable (in sample and out of sample erros both approach 0 as m increases) with generalization error dipping to below 10^-4 at lowest point, which makes sense according to VC theory since if both in-sample and generalization errors are low, then out-of-sample error must be low too. When the model complexity is too low (1st scenario) while generalization error may be lower (with lower dimension, the vc dimension is also lower resulting in lower generalization error) than in scenario 3, the in-sample error is higher due to insufficient complexity resulting in higher out-of-sample error, meanwhile, when there's more noise (scenario 3) but sufficient complexity, in-sample error increases (due to noise) but so does generalization error (due to increased dimensionality according to vc theory), so the resulting out-of-sample error is actually quite similar to scenario 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err_vs_m(\n",
    "    d=2,\n",
    "    sigma=0,\n",
    "    f=f_checkers(),\n",
    "    p=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2\n",
    "\n",
    "*The dataset above is generated from the checkboard embedding instead of the polynomial embedding.\n",
    "This implies that the polynomial embedding model is non-realizable for this problem.\n",
    "Therefore, even when there is no randomness in the labeling process,\n",
    "the model still cannot achieve 0 approximation error (i.e. $E_{out}(h)>0$ for all $h \\in E_{out}(h)$.*\n",
    "\n",
    "*Increasing the polynomial degree $p$ reduces the approximation error.\n",
    "Try each value of $p$ starting at 1 until you reach an approximation error<1%.\n",
    "What is that value of $p$?*\n",
    "\n",
    "After running the code at p=25 for over 15 minutes with the test accuracy still increasing incrementally around 0.98xx, we can conclude that p>25, though the exact value remains to be seen due to extremely long computation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err_vs_m(\n",
    "    d=2,\n",
    "    sigma=0,\n",
    "    f=f_circles(),\n",
    "    p=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3\n",
    "\n",
    "*The dataset above is also not realizable, but in a different way than in question 3.2.\n",
    "Once again, your task is to find the exact $p$ value that will achieve an approximation error<0.01%.*\n",
    "\n",
    "This is asking us to achieve approximation error <0.01% --> teset accuracy > 0.9999. However, similar obstacle occured as above with running the code for >10 minutes at p=22 and the test accuracy hovering around 0.9900. So again, we can only conclude for now that p>22. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: plotting the error as a function of the sample size $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_err_vs_d(m, sigma, f, p, max_dim_exponent, num_trials=default_num_trials):\n",
    "    '''\n",
    "    Plots the sample/true/generalization error for the halfspace with polynomial kernel model\n",
    "    as a function of the number of sample training points.\n",
    "    '''\n",
    "    m_buffer = 3\n",
    "    assert(2**max_dim_exponent<max_d)\n",
    "\n",
    "    # these lists store the computer training and test errors\n",
    "    test_errs = []\n",
    "    train_errs = []\n",
    "    \n",
    "    # This is the list of all sample sizes we will train models on and generate train/test errors;\n",
    "    # by adjusting the range(), you can adjust the x-axis in the plots below.\n",
    "    ds = [ 2**i for i in range(0,max_dim_exponent) ]\n",
    "\n",
    "    for d in ds:\n",
    "\n",
    "        # In order to \"smooth\" the plots, we will repeat each experiment multiple times\n",
    "        # as deterimed by the num_trials parameter.\n",
    "        # These lists store the raw results from each trial.\n",
    "        trials_test_accs = []\n",
    "        trials_train_accs = []\n",
    "        \n",
    "        \n",
    "        # loop over each trial\n",
    "        seed_base = 10\n",
    "        time_start = time.time()\n",
    "        for seed in range(seed_base,seed_base+num_trials):\n",
    "            \n",
    "            # generate a training set of size m\n",
    "            # from the same distribution as our test set;\n",
    "            # notice that we must explicitly set a unique seed for each trial so that\n",
    "            # each iteration is actually running on a different training set\n",
    "            S_train = generate_dataset(m=m,d=d,f=f,sigma=sigma,seed=seed)\n",
    "            S_test = generate_dataset(m=m,d=d,f=f,sigma=sigma,seed=seed-1)\n",
    "\n",
    "            try:   \n",
    "                # train a linear model;\n",
    "                # notice that the training currently uses the LogisticRegression model;\n",
    "                # all of the results will be essentially the same using the other linear models as well\n",
    "                # since they all use the same hypothesis class\n",
    "                X, Y = S_train\n",
    "                X = np.apply_along_axis(lambda x: polynomial_kernel_embedding(x,p),1,X)\n",
    "                h_S = sklearn.linear_model.LogisticRegression(solver='liblinear',C=1e1)\n",
    "                #h_S = sklearn.linear_model.Perceptron()\n",
    "                #h_S = sklearn.linear_model.SGDClassifier()\n",
    "                #h_S = sklearn.linear_model.PassiveAggressiveClassifier()\n",
    "                #h_S = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "                #h_S = sklearn.svm.LinearSVC()\n",
    "                h_S.fit(X, Y)\n",
    "\n",
    "                # calculate the training accuracy\n",
    "                train_acc = h_S.score(X,Y)\n",
    "\n",
    "                # calculate the test accuracy\n",
    "                X, Y = S_test\n",
    "                X = np.apply_along_axis(lambda x: polynomial_kernel_embedding(x,p),1,X)\n",
    "                test_acc = h_S.score(X, Y)        \n",
    "\n",
    "            # ValueError raised when there's not enough data to perform classification;\n",
    "            # in this case, we get perfect training accuracy, but perfectly wrong test accuracy\n",
    "            except ValueError:\n",
    "                train_acc = 1\n",
    "                test_acc = 0\n",
    "                \n",
    "            trials_test_accs.append(test_acc)\n",
    "            trials_train_accs.append(train_acc)\n",
    "        time_end = time.time()\n",
    "        \n",
    "        # compute the average of our trials\n",
    "        train_acc = np.mean(trials_train_accs)\n",
    "        test_acc = np.mean(trials_test_accs)\n",
    "            \n",
    "        # print a debugging statement for each iteration\n",
    "        print('d=%8d,  train_acc=%0.4f,  test_acc=%0.4f,  time_diff=%dsec'%(\n",
    "            d,\n",
    "            train_acc,\n",
    "            test_acc,\n",
    "            time_end-time_start\n",
    "        ))\n",
    "\n",
    "        # convert the accuracies into errors and store them\n",
    "        train_errs.append(1-train_acc)\n",
    "        test_errs.append(1-test_acc)\n",
    "    \n",
    "    # plot the errors\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(14,5))\n",
    "    ax1.set_xscale('log',base=2)\n",
    "    #ax1.set_yscale('log')\n",
    "    ax1.set(\n",
    "        xlabel='number of dimensions = d', \n",
    "        ylabel='train error = E_in(g)',\n",
    "    )\n",
    "    ax1.plot(ds,train_errs)\n",
    "    \n",
    "    ax2.set_xscale('log',base=2)\n",
    "    #ax2.set_yscale('log')\n",
    "    ax2.set(\n",
    "        xlabel='number of dimensions = d', \n",
    "        ylabel='test error ≈ E_out(g)',\n",
    "    )\n",
    "    ax2.plot(ds,test_errs)\n",
    "    \n",
    "    ax3.set_xscale('log',base=2)\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.set(\n",
    "        xlabel='number of dimensions = d', \n",
    "        ylabel='generalization error = |E_in(g) - E_out(g)|',\n",
    "\n",
    "    )\n",
    "    ax3.plot(ds,np.abs(np.array(test_errs)-np.array(train_errs)))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err_vs_d(\n",
    "    m = 512,\n",
    "    sigma = 0.1,\n",
    "    f = f_polynomial(p=1),\n",
    "    p = 1,\n",
    "    max_dim_exponent=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "\n",
    "*How does increasing the dimension $d$ of the input space affect the errors of the model?\n",
    "How does VC theory predict this behavior?*\n",
    "\n",
    "Increasing the dimension of the input space leads to decrease in in-sample error, increase in generalization error, and (within a reasonable increase) initially decreass E_out as well until it shoots abck up when we increase d so much so that it overfits the data and fails to ignore noise and incorporate it in its pattern recognition. VC theory predicts this behavior since the VC dimension increases as d increases, and while a higher VC dimension may allow the model to capture more complex patterns and reduce training error, VC theory also penalizes overfitting by reflecting the risk of poor generalization in using higher dimension. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err_vs_d(\n",
    "    m = 512,\n",
    "    sigma = 0.1,\n",
    "    f = f_polynomial(p=2),\n",
    "    p = 1,\n",
    "    max_dim_exponent=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2\n",
    "\n",
    "*The plot above is generated using almost the same formula as the plot for 4.1.\n",
    "The exception is that the $f$ function is more complicated (polynomial degree 2 instead of degree 1).\n",
    "What effect does this have on the the errors?*\n",
    "\n",
    "the in-sample error instead of decreasing as d increases (in 4.1) actually increases in 4.2 (and also just generally way higher E-in than in 4.1 for the same d), meanwhile generalization error increases as d increases in a similar fashion as above, and this combined renders a similar trajectory of E-out but with the acrtual value of E-out at each d way higher than it was in 4.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: plotting the error as a function of the polynomial degree\n",
    "\n",
    "### Question 5.1\n",
    "\n",
    "The goal of this question is to demonstrate that large degree polynomial embeddings overfit.\n",
    "This will require writing a new function `calculate_err_vs_p` that is based off of `calculate_err_vs_m` and `calculate_err_vs_d`.\n",
    "Plot the train, test, and generalization errors as a function of `p`.\n",
    "You should find that \"small\" and \"large\" values both generate large test errors,\n",
    "and \"medium\" values of `p` generate small test errors.\n",
    "\n",
    "I recommend using values of `m = 32`, `d = 8`, `sigma = 0.0`, and `f = f_polynomial(p=2)`.\n",
    "Adjust the value of the `p` variable starting at `1` and going large enough to see the pattern of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_err_vs_p(m, sigma, f, d, max_p, num_trials=default_num_trials):\n",
    "    m_buffer = 3\n",
    "\n",
    "    test_errs = []\n",
    "    train_errs = []\n",
    "\n",
    "    ps = list(range(1, max_p + 1))\n",
    "    for p in ps:\n",
    "\n",
    "        # In order to \"smooth\" the plots, we will repeat each experiment multiple times\n",
    "        # as deterimed by the num_trials parameter.\n",
    "        # These lists store the raw results from each trial.\n",
    "        trials_test_accs = []\n",
    "        trials_train_accs = []\n",
    "        \n",
    "        \n",
    "        # loop over each trial\n",
    "        seed_base = 10\n",
    "        time_start = time.time()\n",
    "        for seed in range(seed_base,seed_base+num_trials):\n",
    "            \n",
    "            # generate a training set of size m\n",
    "            # from the same distribution as our test set;\n",
    "            # notice that we must explicitly set a unique seed for each trial so that\n",
    "            # each iteration is actually running on a different training set\n",
    "            S_train = generate_dataset(m=m,d=d,f=f,sigma=sigma,seed=seed)\n",
    "            S_test = generate_dataset(m=m,d=d,f=f,sigma=sigma,seed=seed-1)\n",
    "\n",
    "            try:   \n",
    "                # train a linear model;\n",
    "                # notice that the training currently uses the LogisticRegression model;\n",
    "                # all of the results will be essentially the same using the other linear models as well\n",
    "                # since they all use the same hypothesis class\n",
    "                X, Y = S_train\n",
    "                X = np.apply_along_axis(lambda x: polynomial_kernel_embedding(x,p),1,X)\n",
    "                h_S = sklearn.linear_model.LogisticRegression(solver='liblinear',C=1e1)\n",
    "                #h_S = sklearn.linear_model.Perceptron()\n",
    "                #h_S = sklearn.linear_model.SGDClassifier()\n",
    "                #h_S = sklearn.linear_model.PassiveAggressiveClassifier()\n",
    "                #h_S = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "                #h_S = sklearn.svm.LinearSVC()\n",
    "                h_S.fit(X, Y)\n",
    "\n",
    "                # calculate the training accuracy\n",
    "                train_acc = h_S.score(X,Y)\n",
    "\n",
    "                # calculate the test accuracy\n",
    "                X, Y = S_test\n",
    "                X = np.apply_along_axis(lambda x: polynomial_kernel_embedding(x,p),1,X)\n",
    "                test_acc = h_S.score(X, Y)        \n",
    "\n",
    "            # ValueError raised when there's not enough data to perform classification;\n",
    "            # in this case, we get perfect training accuracy, but perfectly wrong test accuracy\n",
    "            except ValueError:\n",
    "                train_acc = 1\n",
    "                test_acc = 0\n",
    "                \n",
    "            trials_test_accs.append(test_acc)\n",
    "            trials_train_accs.append(train_acc)\n",
    "        time_end = time.time()\n",
    "        \n",
    "        # compute the average of our trials\n",
    "        train_acc = np.mean(trials_train_accs)\n",
    "        test_acc = np.mean(trials_test_accs)\n",
    "            \n",
    "        # print a debugging statement for each iteration\n",
    "        print('p=%8d,  train_acc=%0.4f,  test_acc=%0.4f,  time_diff=%dsec'%(\n",
    "            p,\n",
    "            train_acc,\n",
    "            test_acc,\n",
    "            time_end-time_start\n",
    "        ))\n",
    "\n",
    "        # convert the accuracies into errors and store them\n",
    "        train_errs.append(1-train_acc)\n",
    "        test_errs.append(1-test_acc)\n",
    "    \n",
    "    # plot the errors\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(14,5))\n",
    "    ax1.set_xscale('log',base=2)\n",
    "    #ax1.set_yscale('log')\n",
    "    ax1.set(\n",
    "        xlabel='number of polynomial degree = p', \n",
    "        ylabel='train error = E_in(g)',\n",
    "    )\n",
    "    ax1.plot(ps,train_errs)\n",
    "    \n",
    "    ax2.set_xscale('log',base=2)\n",
    "    #ax2.set_yscale('log')\n",
    "    ax2.set(\n",
    "        xlabel='number of polynomial degree = p', \n",
    "        ylabel='test error ≈ E_out(g)',\n",
    "    )\n",
    "    ax2.plot(ps,test_errs)\n",
    "    \n",
    "    ax3.set_xscale('log',base=2)\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.set(\n",
    "        xlabel='number of polynomial degree = p', \n",
    "        ylabel='generalization error = |E_in(g) - E_out(g)|',\n",
    "\n",
    "    )\n",
    "    ax3.plot(ps,np.abs(np.array(test_errs)-np.array(train_errs)))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
